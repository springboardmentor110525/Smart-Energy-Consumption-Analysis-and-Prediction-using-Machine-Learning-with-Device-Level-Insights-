# -*- coding: utf-8 -*-
"""energy_consumption_analysis_and_prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1akNUw7emLIrw45KvvB7IDT2qpUYMPH-d
"""

#Uploads the smart home energy dataset from the local system into Google Colab for analysis.
#importing dataset
from google.colab import files
uploaded=files.upload()

#importing libraries
#Importing essential Python libraries for data manipulation, numerical operations, and visualization
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
#reading csv file
#Loads the Smart Home Energy Consumption dataset into a pandas DataFrame for processing
df=pd.read_csv('Smart Home Energy Consumption Optimization.csv')

#features in dataset
#Displays the first few rows to understand the structure and contents of the dataset
df.head()

#data types
#Shows the data types and non-null counts of each column to verify schema correctness
df.info()

#checking null values
#Identifies missing values in each column to assess data completeness
df.isnull().sum()

#checking duplicate values
#Checks whether duplicate rows exist in the dataset
df.duplicated().sum()

#converting timestamp to datetime
#Converts the timestamp column to datetime format and sorts records chronologically for time-series analysis
df['timestamp'] = pd.to_datetime(df['timestamp'])
df = df.sort_values('timestamp')
print(df['timestamp'])

#scatter plot using matplotlib library
#Visualizes power consumption over time to identify abnormal or extreme energy usage values
plt.figure(figsize=(12,5))
plt.scatter(df['timestamp'], df['power_watt'], s=10)
plt.xlabel("Timestamp")
plt.ylabel("Power Usage")
plt.title("Scatter Plot for Outlier Detection")
plt.show()

#bar chart for understanding which device consumes how much energy
#Shows total energy consumption by each device type to identify high energy-consuming appliances
device_usage = df.groupby('device_type')['power_watt'].sum()
plt.figure(figsize=(10,5))
plt.title("Device-wise Energy Usage")
plt.xlabel("devices")
plt.ylabel("power")
plt.bar(device_usage.index,device_usage.values,color='pink')
plt.show()

#room wise energy consumption using pie chart to show proportions
#Displays the proportion of energy consumed by different rooms in the house
room_usage=df.groupby('room')['power_watt'].sum()
plt.figure(figsize=(5,5))
plt.title("room wise energy consumption")
plt.pie(room_usage.values, labels=room_usage.index, autopct='%1.1f%%', startangle=90)
plt.show()

#resampling the data hourly/daily to understand hourly patterns
#Aggregates energy consumption into hourly and daily intervals to analyze time-based usage patterns
hourly_df = df.resample('h', on='timestamp')['power_watt'].sum().reset_index()
daily_df = df.resample('D', on='timestamp')['power_watt'].sum().reset_index()
print(hourly_df)
print(daily_df)

#normalization / scaling to balance the values using minmax scaler function
#Scales daily energy consumption values between 0 and 1 to prepare data for machine learning models
from sklearn.preprocessing import MinMaxScaler
scaler=MinMaxScaler()
daily_df['scaled_power']=scaler.fit_transform(daily_df[['power_watt']])
print(daily_df['scaled_power'])

#train test split to test and validate an ml model
#Splits the dataset into training and testing sets to evaluate machine learning model performance
X=df.drop('power_watt',axis=1)
y=df['power_watt']

#Extracting time based features to analyse weekly,monthly,hourly trends
#Derives hour, day, weekday, week number, and month from timestamps to capture temporal patterns
df['hour'] = df['timestamp'].dt.hour
df['day'] = df['timestamp'].dt.day
df['day_of_week'] = df['timestamp'].dt.dayofweek   # 0 = Monday
df['week'] = df['timestamp'].dt.isocalendar().week
df['month'] = df['timestamp'].dt.month

#aggregating device level consumption to understand how much each device is consuming the energy
#Hour-wise Energy Usage
#Analyzes average power consumption for each hour of the day to identify peak usage times
hourly_usage = df.groupby('hour')['power_watt'].mean()
plt.figure()
hourly_usage.plot(kind='line')
plt.xlabel('Hour of Day')
plt.ylabel('Average Power (W)')
plt.title('Hourly Energy Consumption Pattern')
plt.show()

#Day-of-Week Consumption
#Insight: Weekday vs weekend behavior
#Compares average energy usage across weekdays to study behavioral differences
df_usage = df.groupby('day_of_week')['power_watt'].mean()
plt.figure()
df_usage.plot(kind='bar')
plt.xlabel('Day of Week (0=Mon)')
plt.ylabel('Average Power (W)')
plt.title('Day-wise Energy Consumption')
plt.show()

#Monthly Trend
#Insight: Seasonal patterns
#Shows how energy consumption varies across months to observe seasonal patterns
monthly_usage = df.groupby('month')['power_watt'].mean()
plt.figure()
monthly_usage.plot(kind='line', marker='o')
plt.xlabel('Month')
plt.ylabel('Average Power (W)')
plt.title('Monthly Energy Consumption Trend')
plt.show()

#Device-wise Total Consumption
#Highlights total power consumed by each device type for usage comparison
device_energy = df.groupby('device_type')['power_watt'].sum()
plt.figure(figsize=(10,5))
device_energy.plot(kind='bar')
plt.xlabel('Device Type')
plt.ylabel('Total Power Consumption')
plt.title(' Energy Consuming Devices')
plt.xticks(rotation=45)
plt.show()

#creating  lag features
#Generates past energy consumption values to help the model learn time-series dependencies
df['lag_1'] = df['power_watt'].shift(1)
df['lag_2'] = df['power_watt'].shift(2)
df['lag_24'] = df['power_watt'].shift(24)   # if hourly data

#lag_1 visualization
#Shows the relationship between current energy usage and the immediately previous value
plt.figure()
plt.scatter(df['lag_1'], df['power_watt'])
plt.xlabel('Previous Power (Lag-1)')
plt.ylabel('Current Power')
plt.title('Lag-1 vs Current Energy Consumption')
plt.show()

#lag_2 visualization
#Visualizes how energy consumption depends on values from two previous time steps
plt.figure()
plt.scatter(df['lag_2'], df['power_watt'])
plt.xlabel('Previous Power (Lag-2)')
plt.ylabel('Current Power')
plt.title('Lag-2 vs Current Energy Consumption')
plt.show()

#lag_24 visualization for hourly data
#Analyzes daily dependency by comparing current energy usage with the previous day’s consumption
plt.figure()
plt.scatter(df['lag_24'], df['power_watt'])
plt.xlabel('Previous Power (Lag-24)')
plt.ylabel('Current Power')
plt.title('Lag-24 vs Current Energy Consumption')
plt.show()

#Creating Moving Averages
#Goal: Capture smooth trends and reduce noise
#Calculates rolling averages to smooth short-term fluctuations and capture long-term trends
df['rolling_mean_3'] = df['power_watt'].rolling(3).mean().shift(1)
df['rolling_mean_7'] = df['power_watt'].rolling(7).mean().shift(1)
df = df.dropna()

#Rolling Average Visualization
#Compares actual energy consumption with smoothed trends to visualize noise reduction
plt.figure()
plt.plot(df.index, df['power_watt'], label='Actual')
plt.plot(df.index, df['rolling_mean_7'], label='7-Day Rolling Mean')
plt.legend()
plt.xlabel('Time')
plt.ylabel('Power (W)')
plt.title('Energy Consumption Trend with Rolling Average')
plt.show()

#Histogram of Engineered Features
#Visualizes the distribution of lag and rolling features to understand data spread and behavior
df[['lag_1', 'lag_2','lag_24','rolling_mean_3', 'rolling_mean_7']].hist(figsize=(10,10))
plt.show()

#Final Feature Set for ML Input
#Selects engineered features required as input (X) and target (y) for machine learning models
features = [
    'hour', 'day_of_week', 'month',
    'lag_1', 'lag_2', 'lag_24',
    'rolling_mean_3', 'rolling_mean_7'

]

X = df[features]
from sklearn.preprocessing import MinMaxScaler

target_scaler = MinMaxScaler()
df['power_scaled'] = target_scaler.fit_transform(df[['power_watt']])

y = df['power_scaled']

from sklearn.model_selection import train_test_split
X = df[features]
y = df['power_scaled']
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, shuffle=False
)

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
pipeline_lr = Pipeline([
    ('scaler', StandardScaler()),
    ('model', LinearRegression())
])

pipeline_lr.fit(X_train, y_train)
y_pred = pipeline_lr.predict(X_test)

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np

mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)
print("MAE:", mae)
print("RMSE:", rmse)
print("R2 Score:", r2)

plt.figure(figsize=(7,7))

# Scatter plot
plt.scatter(y_test, y_pred, alpha=0.6, label='Predicted vs Actual')

# Perfect prediction line (y = x)
min_val = min(y_test.min(), y_pred.min())
max_val = max(y_test.max(), y_pred.max())
plt.plot([min_val, max_val], [min_val, max_val], 'r--', label='Ideal Fit (y = x)')

plt.xlabel('Actual Energy Usage (W)')
plt.ylabel('Predicted Energy Usage (W)')
plt.title('Linear Regression: Actual vs Predicted Energy Usage')
plt.legend()
plt.grid(True)
plt.show()

#Module 5
#LSTM Model Development
#In this step, required Python libraries such as NumPy, Pandas, TensorFlow/Keras, and evaluation metrics are imported.
#These libraries are used for data handling, building the LSTM neural network, training the model, and evaluating its performance.
#NumPy → numerical operations
#TensorFlow/Keras → deep learning model
#Scikit-learn → metrics and scaling

#This cell extracts the energy consumption (power_watt) column from the dataset and converts it into a NumPy array.
#This prepares the target variable in a format suitable for scaling and time-series modeling using LSTM.
import numpy as np
data = df[['power_scaled']].values

#This cell applies Min-Max normalization to scale the energy consumption values between 0 and 1.
#Normalization is important for LSTM models as it improves training stability and convergence.
train_size = int(0.8 * len(data))
train_data = data[:train_size]
test_data  = data[train_size:]

#This function converts the time series data into sequences using a sliding window approach,
#which is required to provide 3D input to the LSTM model.
#This cell defines a function to convert the scaled energy data into sequential input-output pairs using a sliding window approach.
#The model uses the previous 24 time steps to predict the next energy value, which converts the data into a 3D format required by LSTM.

TIME_STEPS = 24

def create_sequences(data, time_steps):
    X, y = [], []
    for i in range(time_steps, len(data)):
        X.append(data[i-time_steps:i])
        y.append(data[i])
    return np.array(X), np.array(y)

X_train, y_train = create_sequences(train_data, TIME_STEPS)
X_test, y_test   = create_sequences(test_data, TIME_STEPS)

#This cell splits the dataset into training and testing sets while maintaining the temporal order.
#Shuffling is avoided because LSTM models require sequential data to learn time-based patterns correctly.

split = int(0.8 * len(X))
X_train, X_test = X[:split], X[split:]
y_train, y_test = y[:split], y[split:]

#This cell defines and compiles the LSTM neural network architecture for energy consumption prediction.
 #The model consists of two stacked LSTM layers to capture both short-term and long-term temporal dependencies in the time-series data.
 # A Dropout layer is added to reduce overfitting by randomly disabling neurons during training. The final Dense layer outputs a single predicted energy value.
#The model uses the Adam optimizer and Mean Squared Error (MSE) loss function, which is suitable for regression-based forecasting problems.

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
model = Sequential([
    LSTM(64, return_sequences=True, input_shape=(TIME_STEPS, 1)),
    Dropout(0.2),
    LSTM(32),
    Dense(1)
])

model.compile(
    optimizer='adam',
    loss='mse'
)

#This cell trains the LSTM model using the training dataset.
#A portion of the training data is used for validation to monitor model performance and reduce overfitting during training.

from tensorflow.keras.callbacks import EarlyStopping

early_stopping = EarlyStopping(
    monitor='val_loss',
    patience=5,
    restore_best_weights=True
)

history = model.fit(
    X_train, y_train,
    epochs=3,
    batch_size=32,
    validation_split=0.1,
    callbacks=[early_stopping],
    verbose=1
)

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

y_pred = model.predict(X_test)

lstm_mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
r2  = r2_score(y_test, y_pred)

print("LSTM (Single Split)")
print("MAE:", lstm_mae)
print("MSE:", mse)
print("R2:", r2)

import pandas as pd
import numpy as np

results = pd.DataFrame({
    "Model": ["Linear Regression", "LSTM"],
    "MAE": [mae, lstm_mae],
    "RMSE": [rmse, np.sqrt(mse)],
    "R2 Score": [r2, r2]
})

results

best_model = results.sort_values(
    by=["R2 Score", "RMSE"],
    ascending=[False, True]
).iloc[0]

print("✅ Best Model:")
print(best_model)

model.save("lstm_energy_model.h5")

import joblib

model_metadata = {
    "model_type": "LSTM",
    "time_steps": TIME_STEPS,
    "epochs": 3,
    "batch_size": 32,
    "optimizer": "adam",
    "loss_function": "mse",
    "metrics": {
        "MAE": float(lstm_mae),
        "RMSE": float(np.sqrt(mse)),
        "R2": float(r2)
    },
    "scalers": {
        "target_scaler": target_scaler
    }
}

joblib.dump(model_metadata, "lstm_metadata.joblib")

print("✅ Model metadata saved")

from google.colab import files

files.download("lstm_energy_model.h5")
files.download("lstm_metadata.joblib")